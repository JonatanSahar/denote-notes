#+title:      Presentations prep
#+date:       [2023-04-09 Sun 13:05]
#+filetags:   :bib:
#+identifier: 20230409T130534


* Sagol seminar
1. Introduction (1-2 minutes)
   - who I am, a word about the lab and motor learning
   - a word about sensorimotor integration
   - briefly introduce the topic:
     + what is sensory modulation? Daily life examples.
     + theoretical model: Efference copy and the forward model.
   - key questions and the research aims:
     + What is the difference in modulation of the auditory cortex between ipsi and contra-lateral hand-ear configuration.
     + The mechanism of the effect of action on the processing of the action's consequences
     + Crosstalk between sensory and motor circuits
     + Sensorimotor integration - in order for integration to happen, we must differentiate the results of our actions from unrelated effects in the environment.
2. Literature review (2-3 minutes)
   Pretty much all of our interactions with the world are based in motor action. Furthermore, almost all of these interactions are aimed at producing some effect in the world - an effect which is perceivable by our own senses.
   The process of learning (during development and as adults alike) to act in a way that matches our wants is a process of/a process that builds upon sensorimotor integration: the ability of the CNS to process incoming sensory stimuli, and coordinate them with its own motor commands.
   A cornerstone/a prerequisite of sensorimotor integration is the ability to distinguish and differentially process the sensory consequences of our own actions. Indeed there is a large body of research that demonstrates the ways in which we perceive the same stimulus differently when it's self generated as opposed to passively perceived.
   - Some examples from our lab:
     + Buaron 2020: stronger perceptual effect (active vs passive stimulus).
     + Reznik 2014: lower (binaural) hearing threshold for self generated sounds
     + Reznik 2021: difference in modulation between conditions - above threshold sounds are attenuated, near threshold sounds are strengthened

   - And also how the brain processes them differently:
     + Reznik 2014: stronger modulation (enhancement of activity) of the auditory cortex in the ipsilateral configuration
     + Reznik 2015: stronger activation in auditory cortex for self generated vs passive stimuli
     + Reznik 2018: stronger readiness potential (RP) for actions coupled to sound vs actions not coupled
     + shows that the sensory consequence is encoded in the action.
     + Reznik 2021: evidence of efference copy: action (time-)locked evoked responses in auditory cortex were observed /before/ sound onset.

     The main theoretical framework that aims to explain this mechanism is called the [[denote:20230410T144059][Forward model]]: suggesting that when a motor command leaves the motor cortex to the brainstem and beyond, information about the command, termed an [[denote:20230402T112858][Efference copy]] is transmitted over to (the relevant? Some evidence for this, see David Schneider's papers: [cite:@schneiderHowMovementModulates2018; @schneiderReflectionsActionSensory2020; @audettePreciseMovementbasedPredictions2022]) sensory cortices, where it's used to anticipate the action's sensory consequences, and drives the predictions against which prediction errors can be detected and used to update the movement and/or the internal model.

     This efference copy arrives at the sensory cortex before the onset of the stimulus and so can affect its processing.

     - Several studies in the lab demonstrated that the lateral configuration of the affector (e.g the hand producing the stimulus/sound) and the effector (the sense organ receiving it) is of significance:
       + Buaron 2020: in the visual domain, ipsilateral configuration creates stronger neural modulation
       + Buaron 2020: it's possible to decode the active hand from auditory cortex ROI
       + Reznik 2014: lower monaural threshold in the ipsilateral configuration
       + Reznik 2015: stronger activation in ipsilateral vs contralateral configuration
       + Dery (in prep): better learning of a motor sequence+timing in the *contralateral* configuration (different from the rest!)

3. Methods (2 minutes)
   - fMRI is acquiring the BOLD response which is known (has been shown) to be well correlated with local field potentials (LFP) as recorded by extracellular electrodes.
   - GLM is an analysis of BOLD responses given a series of timed stimuli of several types. We look at the time course of BOLD signal in a voxel, and at the time course of the different stimuli. For each stimulus type we create a modeled response using an idealized BOLD response function (double gamma HRF). We then use a regression algorithm to find the best coefficients to these modeled response functions such that the residual error is minimal.
     This beta value tells us _how much of the activity in that voxel_ is due to the neurons there responding to each stimulus type.
     We then compare these beta values between conditions to get a measure of how this brain region differentially responds to these conditions.
     *Add a figure with the different time courses and betas etc.*
   - in MVPA (multi voxel pattern analysis) we essentially try to decode information from brain ares. We're asking: given the neural activity in this region, can we determine which external stimulus was presented at that time?
     The areas we decode from are typically "neighborhoods" of 3x3x3 voxels, and the values we get from this method are _classification accuracies_, i.e what was the percent of correct samples (e.g stimuli blocks) that we classified correctly.
     We keep the value in the central voxel, and we can compare the classification accuracy between different areas and different stimuli, and we can also infer about the information content of the activity in that area (like in Buaron 2022, where hand identity was decoded from V1)
4. Experimental design (2 minutes)
   - The experiment is conducted inside the MRI scanner. Subjects hold a response box with R and L buttons, and noise isolating earphones. Each block is 8 seconds long with 8 seconds for signal wash-out. Before each block, subject are instructed with which hand they are to press the button to initiate the sound.
   - Subject use either their L or R hands to trigger sounds, and the sounds are presented monaurally.
   - There are 4 runs, each run is dedicated to a single ear.
   - The design is a 2x2, so we get all 4 combinations of (RH, LH)x(RE, LE)
   - The analysis is done per ear - so that each run can be analyzed separately.
   - For the GLM part, the first-level will compare RH vs LH pressing per run, the second-level will average the results per ear (so averaging two runs per each ear), and the third level will average and check for significance on the group level - averaging across subjects.
5. Results (2-3 minutes)
   So far I only have data from two pilot runs which just shows a sanity check that the auditory and motor cortices are both activated.
6. Conclusion and implications (1-2 minutes)
   - Summarize the main conclusions you drew from your study
   - Discuss the implications of your findings and how they contribute to the field of audiomotor integration
7. Q&A (1-2 minutes)
